{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a generative model i.e. the model can randomly generate the observed data\n",
    "* The goal is to learn 2 probability distributions:\n",
    "    1. Probability of documents belong to various topics ($\\theta$) -- topic-document distribution\n",
    "    2. Probability of words belonging to various topics ($\\phi$) -- word-topic distribution\n",
    "     \n",
    "\n",
    "\n",
    "**References**\n",
    "1. [Latent Dirichlet Allocaiton Under the Hood - Andrew Brooks](http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:52:50.347698Z",
     "start_time": "2020-01-06T16:52:50.345162Z"
    }
   },
   "outputs": [],
   "source": [
    "rawdocs = ['eat turkey on turkey day holiday',\n",
    "          'i like to eat cake on holiday',\n",
    "          'turkey trot race on thanksgiving holiday',\n",
    "          'snail race the turtle',\n",
    "          'time travel space race',\n",
    "          'movie on thanksgiving',\n",
    "          'movie at air and space museum is cool movie',\n",
    "          'aspiring movie star']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:52:50.855402Z",
     "start_time": "2020-01-06T16:52:50.852205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'], ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'], ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'], ['snail', 'race', 'the', 'turtle'], ['time', 'travel', 'space', 'race'], ['movie', 'on', 'thanksgiving'], ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'], ['aspiring', 'movie', 'star']]\n"
     ]
    }
   ],
   "source": [
    "docs = [x.split(' ') for x in rawdocs]\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:52:51.501708Z",
     "start_time": "2020-01-06T16:52:51.498697Z"
    }
   },
   "outputs": [],
   "source": [
    "## PARAMETERS\n",
    "num_topics = 2 # number of topics\n",
    "alpha = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "eta = .001 # hyperparameter\n",
    "iterations = 3 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:52:53.023497Z",
     "start_time": "2020-01-06T16:52:52.825232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 25, 14, 25, 6, 8], [9, 11, 22, 7, 4, 14, 8], [25, 24, 15, 14, 19, 8], [16, 15, 20, 26], [21, 23, 17, 15], [12, 14, 19], [12, 3, 0, 1, 17, 13, 10, 5, 12], [2, 12, 18]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "## Assign WordIDs to each unique word\n",
    "\n",
    "# vocab = np.unique(unlist(docs))\n",
    "vocab = list(np.unique([word for doc in docs for word in doc]))\n",
    "\n",
    "\n",
    "# ## Replace words in documents with wordIDs\n",
    "transformed_docs = []\n",
    "for doc in docs:\n",
    "    transformed_docs.append([])\n",
    "    for i in doc:\n",
    "        transformed_docs[-1].append(vocab.index(i))\n",
    "        \n",
    "print(transformed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:52:54.092955Z",
     "start_time": "2020-01-06T16:52:54.087120Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "## 1. Initialize word topic count matrix\n",
    "wt = np.zeros((len(vocab), num_topics), dtype=np.int)\n",
    "\n",
    "## Initialize topic assignment list. This is a three way matrix where \n",
    "## row index corresponds to a document\n",
    "## column index corresponds to a word\n",
    "## value will indicate the topic to which the word is assigned to in a given document\n",
    "ta = np.ndarray((len(transformed_docs), len(vocab)), dtype=np.int8)\n",
    "ta[:,:] = -1\n",
    "\n",
    "## We randomly assign topics to words in a given document\n",
    "\n",
    "for doc_index, doc in enumerate(transformed_docs):\n",
    "    \n",
    "    for word_index in doc:\n",
    "        \n",
    "        # randomly sample a topic between 0 and num_topics\n",
    "        topic_index = randint(0, num_topics-1)\n",
    "        \n",
    "        # randomly assign a word that appears in a document a random topic\n",
    "        ta[doc_index, word_index] = topic_index\n",
    "        \n",
    "        # keep track of how often a word is assigned to a given topic\n",
    "        wt[word_index, topic_index] += 1\n",
    "\n",
    "# print(wt)\n",
    "# print(ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:52:57.692844Z",
     "start_time": "2020-01-06T16:52:57.689120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3.]\n",
      " [3. 4.]\n",
      " [2. 4.]\n",
      " [2. 2.]\n",
      " [1. 3.]\n",
      " [2. 1.]\n",
      " [2. 7.]\n",
      " [1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Count how many words of a document belongs to a given topic\n",
    "dt = np.zeros((len(transformed_docs), num_topics))\n",
    "\n",
    "for doc_index, doc in enumerate(transformed_docs):\n",
    "    \n",
    "    for word_index in doc:\n",
    "        \n",
    "        # extract topic assigned to the word\n",
    "        topic_index = ta[doc_index, word_index]\n",
    "        \n",
    "        # count how many words in the doc are assigned to that topic\n",
    "        dt[doc_index, topic_index] += 1\n",
    "        \n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(i, j) = \\frac{wt[i, j]}{\\sum_k{wt[i,k]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-06T16:53:07.309024Z",
     "start_time": "2020-01-06T16:53:07.302621Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a2c58f8cfd74>, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a2c58f8cfd74>\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    t1 = sample(1:K, 1, prob=pz/sum(pz))\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "for i in range(num_epoch): # number of iterations\n",
    "    \n",
    "    \n",
    "    for doc_index, doc in enumerate(transformed_docs): # for each doc\n",
    "    \n",
    "        for word_index in doc: # for each word token in the doc\n",
    "            \n",
    "            # current topic assigned to the word\n",
    "            t0 = ta[doc_index, word_index]\n",
    "            \n",
    "            # we don't want to include token w in our document-topic count matrix when sampling for token w\n",
    "            dt[doc_index, t0] -= 1\n",
    "            \n",
    "            # we don't want to include token w in our word topic count matrix when sampling for token w \n",
    "            wt[word_index, t0] -= 1\n",
    "            \n",
    "            \n",
    "            ## UPDATE TOPIC ASSIGNMENT FOR EACH WORD\n",
    "            ## -- COLLAPSED GIBBS SAMPLING MAGIC.  Where the magic happens.\n",
    "            denom_a = np.sum(dt[doc_index, :]) + (num_topics * alpha)\n",
    "            denom_b = np.sum(wt, axis=0) + (vocab_size * eta)\n",
    "            pz = (wt[:, word_index] + eta) / denom_b * (dt[d, :] + alpha) / denom_a\n",
    "            t1 = \n",
    "            t1 = sample(1:K, 1, prob=pz/sum(pz))\n",
    "            \n",
    "#           # denom_a <- sum(dt[d,]) + K * alpha # number of tokens in document + number topics * alpha\n",
    "            # denom_b <- rowSums(wt) + length(vocab) * eta # number of tokens in each topic + # of words in vocab * eta\n",
    "            #  p_z <- (wt[,wid] + eta) / denom_b * (dt[d,] + alpha) / denom_a # calculating probability word belongs to each topic\n",
    "            # t1 <- sample(1:K, 1, prob=p_z/sum(p_z)) # draw topic for word n from multinomial using probabilities calculated above\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T17:10:29.859084Z",
     "start_time": "2019-12-12T17:10:29.854359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.],\n",
       "       [-1., -1., -1., -1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ndarray((3,4))\n",
    "x[:,:] = -1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
