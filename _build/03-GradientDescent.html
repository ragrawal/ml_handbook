---
redirect_from:
  - "/03-gradientdescent"
interact_link: content/03-GradientDescent.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Basic Intution (Part I)
prev_page:
  url: /02-ClassificationMetrics.html
  title: |-
    Classification
next_page:
  url: /04-LinearRegression.html
  title: |-
    Linear Regression (Part II)
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Gradient-Descent:-Part-I">Gradient Descent: Part I<a class="anchor-link" href="#Gradient-Descent:-Part-I"> </a></h1>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One cannot overstate the importance "Gradient descent" in the field of machine learning. It's an optimization technique that lies in the heart of machine learning algorithm ranging from regression to deep learning. This chapter aims to provide intuition behind the gradient descent algorithm and it's variant. Similar to previous chapters, I work through examples and code in order to build a concrete understanding of the concept.</p>
<h2 id="Finding-Minimum-Using-Brute-Force">Finding Minimum Using Brute Force<a class="anchor-link" href="#Finding-Minimum-Using-Brute-Force"> </a></h2><p>Assume a two dimensional function $f(X)=x_0^2 + x_1^2$. Now let's assume we want to find the value of $X=(x_0, x_1)$ at which f(X) is minimum. A natural instinct would be to plot the function for some random range of X and observe the behavior of $f(X)$. Using plotly, below code snippet renders the above function.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">copy</span>
<span class="kn">import</span> <span class="nn">itertools</span>

<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;the function we are trying to minimize&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">point_generator</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="n">func</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Generates all possible combinations of X and returns X and f(x) values&quot;&quot;&quot;</span>
    
    <span class="k">for</span> <span class="n">pt</span> <span class="ow">in</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">pt</span><span class="p">)</span>
        <span class="k">yield</span><span class="p">(</span><span class="n">pt</span> <span class="o">+</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="n">pt</span><span class="p">)])</span>
        
        <span class="n">pt</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span>
        <span class="k">yield</span><span class="p">(</span><span class="n">pt</span> <span class="o">+</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="n">pt</span><span class="p">)])</span>
        
    <span class="c1"># generate diagonals</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">choices</span><span class="p">:</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span>
        <span class="k">yield</span><span class="p">(</span><span class="n">pt</span> <span class="o">+</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="n">pt</span><span class="p">)])</span>
    
    
<span class="n">choices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span><span class="o">/</span><span class="mf">10.</span>
<span class="n">df</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pd</span>
    <span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">point_generator</span><span class="p">(</span><span class="n">choices</span><span class="p">,</span> <span class="n">func</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x0&#39;</span><span class="p">,</span> <span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;fx&#39;</span><span class="p">])</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>

<span class="kn">from</span> <span class="nn">chart_studio.plotly</span> <span class="k">import</span> <span class="n">plot</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter_3d</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x0&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="s1">&#39;fx&#39;</span><span class="p">,</span> <span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">to_image</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># fig.show() -- use this command for generating interactive image</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x0</th>
      <th>x1</th>
      <th>fx</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>-50.0</td>
      <td>-49.0</td>
      <td>4901.0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>-49.0</td>
      <td>-50.0</td>
      <td>4901.0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>-50.0</td>
      <td>-48.0</td>
      <td>4804.0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>-48.0</td>
      <td>-50.0</td>
      <td>4804.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/03-GradientDescent_2_1.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the above plot, one can easily observe that $f(X)$ reaches a minimum value when $X = [0, 0]$ (i.e $x_1 = 0$ and $x_2 = 0$).</p>
<p>As humans, we can do this naturally as long as we can visualize the function.  However, a typical machine learning problem will involve hundreds of features. So let's think how did we so easily find the point X at which $f(X)$ is minimum. One possible explanation is that our eyes randomly picked a point on the above surface and followed in the direction where the slope is decreasing. Finally, when the slop starts increasing again we know that we have crossed the minimum point. Let's try to transfer this idea into a proper python code. The logical flow of the problem will be as follows:</p>
<p><strong>Step 1:</strong> Pick a random point $X = (x_0, x_1)$
<strong>Step 2:</strong> Identify the next point $X'$ such as that $f(X') &lt; f(X)$. However, one challenge is how do we find $X'$. Let's start with some brute force. Assume we look for neighboring points that are $\eta$ distance away on a single dimension.  For instance, as shown below figure (a), let's assume $X=[1,1]$ and $\eta = 0.25$. Then, the neighboring points to explore are [1-0.25, 1], [1 + 0.25, 1], [1, 1-0.25], [1, 1 + 0.25]. Thus, if X is n dimensional vector then there will be $2^n$ neighbors to explore. If we relax the constraint that one can only move $\eta$ distance along on one dimension at any given time, then we have additional four diagonal points as shown in the figure: [1-0.25, 1-0.25], [1+0.25, 1-0.25], [1-0.25, 1+0.25], [1+0.25, 1+0.25]. For n dimensional space then there will be $3^n$ neighboring points.</p>
<p><img src="images/Chapter3_NeighboringPoints.png" alt=""></p>
<p>Ignoring the scalability issue for now, let's assume we do use the above approach to generate possible neighboring candidates at which we need to evaluate $f(X)$.</p>
<p><strong>Step 3:</strong> Of all neighboring points at which $f(X') &lt; f(X)$, pick the one with the maximum difference and set this new point around which we explore the space.</p>
<p><strong>Step 4</strong>: Repeat step 2 and 3 until one reaches the point where $f(X') &gt; f(X)$ for all neighboring point.</p>
<p>The below python code implements the above logic. It assumes $\eta$ to be 0.03</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">namedtuple</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="k">import</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># we will use Point class to store x and assocated y values</span>
<span class="n">Point</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s1">&#39;Point&#39;</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;the function we are trying to minimize&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Point</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_x</span> <span class="o">=</span> <span class="n">X</span>
        
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">x</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_x</span>
    
    <span class="k">def</span> <span class="nf">neighbors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">distance</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;For a given point, generate all the neighbors </span>
<span class="sd">            by walking delta distance along a single dimension only</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="n">pts</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">distance</span><span class="p">,</span>
                <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">distance</span>
            <span class="p">]</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">Point</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">pts</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())]</span>
    
    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__eq__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">==</span> <span class="n">other</span><span class="o">.</span><span class="n">x</span>
        

<span class="k">def</span> <span class="nf">minimize_v1</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">start_point</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">traceback</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    
    <span class="n">x1</span> <span class="o">=</span> <span class="n">Point</span><span class="p">(</span><span class="n">start_point</span><span class="p">)</span>
    <span class="n">y1</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>    
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">traceback</span><span class="p">:</span>
            <span class="n">traceback</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>


        <span class="c1"># generate neighbors at distance of eta</span>
        <span class="n">neighbors</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">neighbors</span><span class="p">(</span><span class="n">eta</span><span class="p">)</span>
        
        <span class="c1"># Identify next point where the f(x) is smallest</span>
        <span class="n">min_x</span> <span class="o">=</span> <span class="n">x1</span>
        <span class="n">min_y</span> <span class="o">=</span> <span class="n">y1</span>     
        
        <span class="k">for</span> <span class="n">x2</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">:</span>    
            <span class="n">y2</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">y2</span> <span class="o">&lt;</span> <span class="n">min_y</span><span class="p">:</span>
                <span class="n">min_x</span><span class="p">,</span> <span class="n">min_y</span> <span class="o">=</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span>
        
        
        <span class="c1"># if x2 is same as x1 then we are at minimum</span>
        <span class="k">if</span> <span class="n">x1</span> <span class="o">==</span> <span class="n">min_x</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x1</span><span class="o">.</span><span class="n">x</span>
        
        <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">min_x</span><span class="p">,</span> <span class="n">min_y</span>
        
    <span class="c1"># if we reached max iterations then return current point</span>
    <span class="k">return</span> <span class="n">x1</span><span class="o">.</span><span class="n">x</span>
    

<span class="n">minPt</span> <span class="o">=</span> <span class="n">minimize_v1</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&lt;strong&gt;Optimal </span><span class="si">{}</span><span class="s2">&lt;/strong&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">minPt</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it 
<span class="n">minimize_v1</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-Gradient-Approach">Using Gradient Approach<a class="anchor-link" href="#Using-Gradient-Approach"> </a></h2><p>Great, now we have working function. But it's not scalable. If you have n dimensionals pace, there are $3^n$ possible neighbors that we need to explore (this is assuming that we are taking $\eta$ distance along each dimension). One way to solve the above scalability issue is use the concept of directional derivative and gradients.</p>
<p>The main objective of step 2 is to find $X'$ for which $f(X')$ is smallest. This is exactly the kind of information that derivative provides. For instance, our function $f(X)=x_0^2 + x_1^2$ has two parameters: $x_0, x_1$. So partial derivative of our function $f(x)$, written as $\Delta{f(X)}$ is given as:</p>
$$\Delta{f(X)} = \begin{bmatrix}
\frac{\partial{f}}{\partial{x_0}}\\ 
\frac{\partial{f}}{\partial{x_1}}
\end{bmatrix} = \begin{bmatrix}
2x_0\\
2x_1
\end{bmatrix}$$<p>The partial derivative gives the direction of steepest gradient. Since we are interested in minimization, we want to move opposite to the direction of greatest gradient. Thus, rather than having to explore $3^n$ points, we can essentially find $X'$ using the below formula</p>
$$ X' = X - \eta \Delta{f(X)}$$<p>There are two things to note in the above equation:</p>
<ol>
<li>$\Delta{f(X)}$ gives the direction of steepest gradient. Since we want to minimize, we want to go in the opposite direction of steepest gradient and hence "minus" sign. </li>
<li>$\Delta{f(X)}$ is a unit vector. If we directly do $X - \Delta{f(X)}$ then we are finding neighbors in opposite direction of steepest gradient that is at a distance of one unit from the current point X. A jump of one unit might be too much and hence we dampen it by multipling by $\eta$. Often <strong>$\eta$ is referreed as the learning rate</strong>.</li>
</ol>
<p>Using the gradient approach, let's write the second version of minimization method. Note that below, the func now doesn't refer to the original function ($x_0^2 + x_1^2$) but to it's derivative i.e. ($2x_0 + 2x1$).</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">copy</span>

<span class="k">def</span> <span class="nf">func_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="mf">2.</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">])</span>


<span class="k">def</span> <span class="nf">minimize_v2</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">start_point</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">traceback</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    
    <span class="n">x1</span> <span class="o">=</span> <span class="n">start_point</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">traceback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">traceback</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>

        <span class="n">x1</span> <span class="o">-=</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
        
    <span class="c1"># if we reached max iterations then return current point</span>
    <span class="k">return</span> <span class="n">x1</span>



<span class="n">minPt</span> <span class="o">=</span> <span class="n">minimize_v2</span><span class="p">(</span><span class="n">func_gradient</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&lt;strong&gt;Optimal </span><span class="si">{}</span><span class="s2">&lt;/strong&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">minPt</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it 
<span class="n">minPt</span> <span class="o">=</span> <span class="n">minimize_v2</span><span class="p">(</span><span class="n">func_gradient</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Stopping-Criteria">Stopping Criteria<a class="anchor-link" href="#Stopping-Criteria"> </a></h2><p>if you compare v1 and v2 implementation, you will notice that using the gradient approach significantly simplified our logic. But in theory it should also make it much more scalable and faster. One important detail that I skipped and affects the performance of the gradient based approach is <strong>stopping criteria</strong>. Currently the only way <code>minimize_v2</code> can stop is when it reaches maximum number of iterations. If you plot the traceback, as shown below, you will notice that the x2 doesn't change much after 500 iterations.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">traceback</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">minPt</span> <span class="o">=</span> <span class="n">minimize_v2</span><span class="p">(</span><span class="n">func_gradient</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">traceback</span> <span class="o">=</span> <span class="n">traceback</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Optimal point is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">minPt</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># convert traceback into a data frame for plotting purpose</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">traceback</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="n">value_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">])</span>

<span class="kn">from</span> <span class="nn">plotnine</span> <span class="k">import</span> <span class="o">*</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;iteration&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">250</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">facet_wrap</span><span class="p">(</span><span class="s2">&quot;~variable&quot;</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Iteration Number&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">theme_linedraw</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">theme</span><span class="p">(</span><span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Thus, one way to speed up things is to introduce a stopping criteria. Let's assume we stop as soon as the euclidean distance between x1 and x2 is less than some threshold, say 1e-6. Minimize V3 adds this stopping criteria.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">copy</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="k">import</span> <span class="n">distance</span>

<span class="k">def</span> <span class="nf">func_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="mf">2.</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">])</span>


<span class="k">def</span> <span class="nf">minimize_v3</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">start_point</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">traceback</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">stopping_threshold</span> <span class="o">=</span> <span class="mf">1.0e-6</span><span class="p">):</span>
    
    <span class="n">x1</span> <span class="o">=</span> <span class="n">start_point</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">traceback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">traceback</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>

        <span class="n">x2</span>  <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">func</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>
        
        <span class="c1"># check if we reached stopping criteria threshold</span>
        <span class="k">if</span> <span class="n">distance</span><span class="o">.</span><span class="n">euclidean</span><span class="p">(</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">stopping_threshold</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x2</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x1</span> <span class="o">=</span> <span class="n">x2</span>
        
    <span class="c1"># if we reached max iterations then return current point</span>
    <span class="k">return</span> <span class="n">x1</span>


<span class="n">traceback</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">minPt</span> <span class="o">=</span> <span class="n">minimize_v3</span><span class="p">(</span><span class="n">func_gradient</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">traceback</span><span class="o">=</span><span class="n">traceback</span><span class="p">,</span> <span class="n">stopping_threshold</span><span class="o">=</span><span class="mf">1.0e-6</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&lt;strong&gt;Optimal </span><span class="si">{}</span><span class="s2">&lt;/strong&gt;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">minPt</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)))</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;Total Number of Iterations: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">traceback</span><span class="p">))))</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>it 
<span class="n">minPt</span> <span class="o">=</span> <span class="n">minimize_v3</span><span class="p">(</span><span class="n">func_gradient</span><span class="p">,</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As you can notice above, we reached reasonably close to the optimal point (0, 0) in just 259 iterations and the performance increased 5x. The mean time reduced from 23 ms to 5 ms in my runs.</p>
<h2 id="More-things-to-consider">More things to consider<a class="anchor-link" href="#More-things-to-consider"> </a></h2><p>Gradient descent algorithm is still an active area of research. There are many aspects of the algorithm that influences its performance both in terms of running time and accuracy. For instance, I didn't really talk much about the impact of learning rate on the performance and accuracy of the gradient descent approach. A small learning rate increases running time but improves accuracy and vice-versa. However, learning rate doesn't have to constant. There are many variants of the gradient descent algorithm where learning rate is dynamically modified based on number of iteration, steepness of gradient, etc. Hence, I would recomment think about following things:</p>
<ol>
<li>Different types of stopping criterias</li>
<li>Impact of learning rate on performance and accuracy. </li>
<li>How to make learning rate auto adjust (search for "Momentum" gradient descent)</li>
<li>Automatically deriving partial derivative of a given function (checkout "autograd" library)</li>
</ol>
<p>In the next chapter, I will show this idea of gradient descent is used in linear regression.</p>

</div>
</div>
</div>
</div>

 

