---
redirect_from:
  - "/01-regressionmetrics"
interact_link: content/01-RegressionMetrics.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Regression
prev_page:
  url: /intro.html
  title: |-
    Introduction
next_page:
  url: /02-ClassificationMetrics.html
  title: |-
    Classification
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Regression-Related-Evaluation-Metrics">Regression Related Evaluation Metrics<a class="anchor-link" href="#Regression-Related-Evaluation-Metrics"> </a></h1><p>Regression problems are the problems where we try to make a prediction on a continuous scale. This chapter aims to help develop (or rediscover) the intuition behind some of these popular metrics, namely $R^2$, for evaluating regression models. Using an open source housing dataset, we first build three different models and thereafter intutitively build various metrics that help evaluate the the performance of the three trained models against each other. We start with basic evaluation metrics such as error, absolute error, etc and eventually try to the build the underlying intutition behind $R^2$.</p>
<h2 id="Predicting-House-Price">Predicting House Price<a class="anchor-link" href="#Predicting-House-Price"> </a></h2><p>Let's assume we are tasked to build a model that can predict house prices. Further, we are provided with an open source Boston city dataset. As shown in the code snippet below, we can load the dataset as a <code>pandas</code> dataframe. In the table below, "price" is the variable we aim to predict and treat the rest of the variables as input features. You can find more information about the individual fields in the dataset over <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names">here</a>.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s2">&quot;scipy&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;^internal gelsd&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>


<span class="n">boston</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_boston</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>354</td>
      <td>0.04301</td>
      <td>80.0</td>
      <td>1.91</td>
      <td>0.0</td>
      <td>0.413</td>
      <td>5.663</td>
      <td>21.9</td>
      <td>10.5857</td>
      <td>4.0</td>
      <td>334.0</td>
      <td>22.0</td>
      <td>382.80</td>
      <td>8.05</td>
      <td>18.2</td>
    </tr>
    <tr>
      <td>294</td>
      <td>0.08199</td>
      <td>0.0</td>
      <td>13.92</td>
      <td>0.0</td>
      <td>0.437</td>
      <td>6.009</td>
      <td>42.3</td>
      <td>5.5027</td>
      <td>4.0</td>
      <td>289.0</td>
      <td>16.0</td>
      <td>396.90</td>
      <td>10.40</td>
      <td>21.7</td>
    </tr>
    <tr>
      <td>192</td>
      <td>0.08664</td>
      <td>45.0</td>
      <td>3.44</td>
      <td>0.0</td>
      <td>0.437</td>
      <td>7.178</td>
      <td>26.3</td>
      <td>6.4798</td>
      <td>5.0</td>
      <td>398.0</td>
      <td>15.2</td>
      <td>390.49</td>
      <td>2.87</td>
      <td>36.4</td>
    </tr>
    <tr>
      <td>255</td>
      <td>0.03548</td>
      <td>80.0</td>
      <td>3.64</td>
      <td>0.0</td>
      <td>0.392</td>
      <td>5.876</td>
      <td>19.1</td>
      <td>9.2203</td>
      <td>1.0</td>
      <td>315.0</td>
      <td>16.4</td>
      <td>395.18</td>
      <td>9.25</td>
      <td>20.9</td>
    </tr>
    <tr>
      <td>110</td>
      <td>0.10793</td>
      <td>0.0</td>
      <td>8.56</td>
      <td>0.0</td>
      <td>0.520</td>
      <td>6.195</td>
      <td>54.4</td>
      <td>2.7778</td>
      <td>5.0</td>
      <td>384.0</td>
      <td>20.9</td>
      <td>393.49</td>
      <td>13.00</td>
      <td>21.7</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We randomly split the data into 80/20 i.e 80% data is used for training and 20% of the data is used for testing. Further, let's assume that we train three different models to predict house prices. The three models we will train are:</p>
<ol>
<li><p><strong>Mean Model:</strong> Assume that the only available information is prices of adjoining houses to your house. Now, if someone asks what's the price of a nearby house, one is likely to guess a value close to the "mean" house price of the adjoining houses. It can be shown that when no information is available other than the target variable, using the expected value (or "mean") of the target variable as the predicted value will minimize the total absolute error. Here, I use the term "mean model" to describe such a model. For every data point, "Mean model" returns a constant value equal to the observed mean value of the target variable in the training dataset. 
As you will see later, this model serves as a great baseline for evaluating other models.</p>
</li>
<li><p><strong>Linear Regression Model:</strong> The second model will use linear regression algorithm. Linear regression and its variants (such as Lasso, ElasticNet, etc) are one of the most popular algorithms to deal with regression problems. For the purpose of this chapter, we will use the ordinary least square algorithm.</p>
</li>
<li><p><strong>Xgboost:</strong> The third model is based on the "Xgboost" algorithm. Xgboost trains multiple decision trees where each tree increases the weight depending on the error observed in the previous tree.</p>
</li>
</ol>
<p>For the purpose of this chapter, it's not important to understand how these different algorithms work. The only motivation for training three different models is to help conceptualize and develop an evaluation metric that allows us to compare different models and determine the best. In the same spirit, we also skip feature engineering and hyper-parameter tunning. These steps are critical to building a good quality model. But the focus of this chapter is not to build the best model for house price prediction but on how to evaluate regression models.</p>
<p>Using training data, below code snippt trains the three models and use the same to predict house price for the houses in our test dataset.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="k">import</span> <span class="n">DummyRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="k">import</span> <span class="n">XGBRegressor</span>

<span class="c1"># Split Data Into Training and Test Dataset </span>
<span class="c1"># so that we use the same training and test dataset </span>
<span class="c1"># to train and test three models. Also setting random state </span>
<span class="c1"># so that we can results are reproducible</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">train</span><span class="o">.</span><span class="n">is_copy</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">test</span><span class="o">.</span><span class="n">is_copy</span> <span class="o">=</span> <span class="kc">None</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Dataset: &quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Dataset: &quot;</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># =====================</span>
<span class="c1"># TRAIN MODELS</span>
<span class="c1"># =====================</span>

<span class="c1"># Mean Model</span>
<span class="c1"># It simply returns mean value based on the training dataset </span>
<span class="c1"># as a prediction. Fixing </span>
<span class="n">mean_model</span> <span class="o">=</span> <span class="n">DummyRegressor</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
<span class="n">mean_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">])</span>

<span class="c1"># Linear Regression</span>
<span class="n">median_model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">median_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">])</span>

<span class="c1"># xgboost model</span>
<span class="n">xgboost_model</span> <span class="o">=</span> <span class="n">XGBRegressor</span><span class="p">()</span>
<span class="n">xgboost_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">])</span>

<span class="c1"># =====================</span>
<span class="c1"># GENERATE PREDICTIONS</span>
<span class="c1"># =====================</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">])</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">median_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">])</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;xgboost&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">xgboost_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span><span class="p">])</span>

<span class="c1">## display results</span>
<span class="n">test</span><span class="p">[[</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;regression&#39;</span><span class="p">,</span> <span class="s1">&#39;xgboost&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Training Dataset:  (404, 14)
Test Dataset:  (102, 14)
[23:30:02] WARNING: src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>price</th>
      <th>mean</th>
      <th>regression</th>
      <th>xgboost</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>65</td>
      <td>23.5</td>
      <td>21.841832</td>
      <td>30.873149</td>
      <td>26.025826</td>
    </tr>
    <tr>
      <td>107</td>
      <td>20.4</td>
      <td>21.841832</td>
      <td>21.243959</td>
      <td>20.909090</td>
    </tr>
    <tr>
      <td>311</td>
      <td>22.1</td>
      <td>21.841832</td>
      <td>26.982815</td>
      <td>23.956749</td>
    </tr>
    <tr>
      <td>119</td>
      <td>19.3</td>
      <td>21.841832</td>
      <td>20.880395</td>
      <td>21.371815</td>
    </tr>
    <tr>
      <td>486</td>
      <td>19.1</td>
      <td>21.841832</td>
      <td>19.676509</td>
      <td>17.655367</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Error-Distribution-and-Mean-Absolute-Error">Error Distribution and Mean Absolute Error<a class="anchor-link" href="#Error-Distribution-and-Mean-Absolute-Error"> </a></h2><p>The next challenge is determining the best model of the three or to rank order them based on the quality. However, quality is undefined and we need to define it. The first thing that comes to mind is to examine error i.e. difference between actual house price and the predicted value. Note that the error can be positive if the model underpredicts the value of a given house or vice-versa. Since there are hundreds of houses in the test dataset, as shown in the below plot, we can further examine the distribution of error for each model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># =====================</span>
<span class="c1"># CALCULATE ERROR</span>
<span class="c1"># =====================</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">]</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;regression_model_error&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">]</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;xgboost_model_error&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;xgboost&#39;</span><span class="p">]</span>

<span class="c1"># =====================</span>
<span class="c1"># PLOT DISTRIBUTION OF ERRORS</span>
<span class="c1"># FOR THREE MODELS</span>
<span class="c1"># =====================</span>

<span class="c1"># convert wide format to row format so that it</span>
<span class="c1"># easy to visualize and compute statistical properties</span>
<span class="n">errorDF</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">value_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">,</span> <span class="s1">&#39;regression_model_error&#39;</span><span class="p">,</span> <span class="s1">&#39;xgboost_model_error&#39;</span><span class="p">])</span> 

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span> <span class="nn">plotnine</span> <span class="k">import</span> <span class="o">*</span>
<span class="n">display</span><span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">errorDF</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">))</span> 
    <span class="o">+</span> <span class="n">geom_density</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">theme_xkcd</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">theme</span><span class="p">(</span><span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Error in house price&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">(</span><span class="s2">&quot;Distribution of Error&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/01-RegressionMetrics_5_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea ">
<pre>&lt;ggplot: (305548849)&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The distribution of error tells a lot about each model. For instance, from the above plot, we can examine that xgboost based model has a much smaller standard deviation as compared to other models. But as an evaluation metric, we desire for a single real number that makes comparing and ranking competing models easier.  Unless there is a significant bias in the model, the error is normally distributed (and as we can observe from the above plot). A normal distribution can be described using two parameters: mean and standard deviation. Thus one option to convert the distribution to a single number that serves as an evaluation criterion is to examine mean error for each model and we can name this evaluation criteria as "mean error". The code snippet below computes the mean and standard deviation for error distribution associated with each model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># =====================</span>
<span class="c1"># CALCULATE MEAN AND SD</span>
<span class="c1"># =====================</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean and Standard Deviation&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span>
    <span class="n">errorDF</span>
    <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="s1">&#39;std&#39;</span><span class="p">]})</span>
    <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">((</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Mean and Standard Deviation
</pre>
</div>
</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead tr th {
        text-align: left;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr>
      <th></th>
      <th>variable</th>
      <th colspan="2" halign="left">value</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th>mean</th>
      <th>std</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>xgboost_model_error</td>
      <td>0.315741</td>
      <td>3.499984</td>
    </tr>
    <tr>
      <td>1</td>
      <td>regression_model_error</td>
      <td>0.739362</td>
      <td>5.848302</td>
    </tr>
    <tr>
      <td>0</td>
      <td>mean_model_error</td>
      <td>3.427776</td>
      <td>10.276974</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using "mean error" as an evaluation criterion we can conclude that xgboost based model has the smallest error as compared to other models. However, there is an obvious flaw in our conclusion.  Let's assume that we have a model, say Model X, that always over predict i.e. the error is always negative. In this case, the mean error will be negative and let's assume it's -3. Since -3 is less than the mean error of the xgboost based model, we might wrongly conclude that Model X is better than the xgboost based model.</p>
<p>One might jump to fix the problem by taking absolute of mean error as it will again restore xgboost based model as the best quality model. There is, however, another issue. Assume that there are two houses in our test dataset and a model underestimate one by 10K, i.e. error = 10K, and overestimate the other by 10K, i.e error = -10K. Here, the mean error and absolute mean error will be zero. This model should be the best model we can as the absolute mean error can never be less than zero.  However, this doesn't make sense.</p>
<p>The problem with absolute mean error is that the individual errors are not additive. To make the errors additive we can take absolute error and thereafter take the mean of absolute error. Thus in the above example, the mean absolute error will be $ \frac{|-10| + |10|}{2} = 10$. Note that there is a significant difference between "absolute mean error" and our new evaluation metric "mean absolute error". In the former, we first compute mean and then apply absolute function. In the later, we first apply the absolute function to individual errors and thereafter take the mean. For obvious reasons, this new evaluation metric is referred as "Mean Absolute Error" or MAE. Mathematically MAE can be defined as:</p>
$$MAE = \frac{\sum_{i=1}^{N}|\hat{y_i} - y_i|}{N}$$<p>Let's re-evaluate the quality of our three models using MAE.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Conver error into absolute error</span>
<span class="n">errorDF</span><span class="p">[</span><span class="s1">&#39;absolute_error&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">errorDF</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
<span class="n">display</span><span class="p">(</span>
    <span class="n">errorDF</span>
    <span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;variable&#39;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s1">&#39;absolute_error&#39;</span><span class="p">:</span> <span class="s1">&#39;mean&#39;</span><span class="p">})</span>
    <span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;absolute_error&#39;</span><span class="p">:</span> <span class="s1">&#39;MAE&#39;</span><span class="p">})</span>
    <span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;MAE&#39;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable</th>
      <th>MAE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>xgboost_model_error</td>
      <td>2.492412</td>
    </tr>
    <tr>
      <td>1</td>
      <td>regression_model_error</td>
      <td>4.061419</td>
    </tr>
    <tr>
      <td>0</td>
      <td>mean_model_error</td>
      <td>7.554703</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The unit of MAE is the same as that of the target variable. This makes MAE easy to understand and interpret. However, "mean" is one of the parameters several parameters related to a distribution and is sensitive to outliers. Hence, sometimes instead of taking the mean of absolute error, "median absolute error" is also reported.</p>
<h2 id="$R^2$">$R^2$<a class="anchor-link" href="#$R^2$"> </a></h2><p>While MAE and its variants are easy to interpret, they have one major shortcoming. These metrics focus on the centrality of the error distribution (i.e. mean or median) and ignore the spread of error. For instance, consider two house prediction models both having MAE of \$10K. However, for one of the model the error ranges from \$5K to \$15K and for another it can range from \$0K to \$50K. In this case,the first model might be more desirable due to its smaller spread. When trying to make a decision about which model is the best, dealing with two separate metrics independently is often challenging and hence one need a single metric that can take into account the whole distribution of errors rather than focusing on the centrality of the errror.</p>
<p>In our search for a better metric, let's revisit some of the core questions we are trying to answer as part of our evaluation strategy. One question that's important is understanding what percentage of houses exhibit less than certain absolute error. We can easily answer this by plotting the cumulative distribution of absolute error as shown below.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># For now ignore this. The below description will help understand this eventually</span>
<span class="n">idealModel</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">errorDF</span><span class="o">.</span><span class="n">absolute_error</span><span class="p">),</span> <span class="mf">1.</span><span class="p">]],</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;y1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="s1">&#39;y2&#39;</span><span class="p">]</span>
<span class="p">)</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">()</span> 
    
    <span class="c1"># we use stat_ecdf function to compuate cumulative distribution and plot the same</span>
    <span class="o">+</span> <span class="n">stat_ecdf</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="s1">&#39;absolute_error&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">errorDF</span><span class="p">,</span> <span class="n">geom</span><span class="o">=</span><span class="s1">&#39;step&#39;</span><span class="p">)</span>
    
    <span class="c1"># plot the idea model</span>
    <span class="o">+</span> <span class="n">geom_segment</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y1&#39;</span><span class="p">,</span> <span class="n">xend</span><span class="o">=</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="n">yend</span><span class="o">=</span><span class="s1">&#39;y2&#39;</span><span class="p">),</span> <span class="n">data</span><span class="o">=</span><span class="n">idealModel</span><span class="p">,</span> <span class="n">linetype</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
    
    <span class="c1"># render labels</span>
    <span class="o">+</span> <span class="n">theme_minimal</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Absolute Error&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;Cumulative Percentage of Houses&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">(</span><span class="s2">&quot;Cumulative Distribution of Absolute Error For Various Models&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/01-RegressionMetrics_11_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;ggplot: (307586665)&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can observe that for the xgboost based model, 95% of the houses have less than 10 unit of absolute error. Whereas only about 75% of houses have less than 10 unit of absolute error for the "mean model".</p>
<p>One thing that also stands out in the above plot is the clear hierarchy of lines. As the model get's better, the closer it's moving towards the top left part of the plot. For instance, the xgboost based model, which is the best of the three based on MAE metric, is closest to the top left part of the plot. Similarly, the "mean model", which is the worst, is farthest from the top left point of the plot. There is a reason for such a behavior. Let's think about an ideal model. An ideal model will zero absolute error for 100% of the test cases. Shown by the black dotted line in the above plot, the cumulative distribution plot for this ideal model will overlap with y-axis and extend up to 1.  Thus, as our models will get better the closer they should move towards the cumulative distribution of the ideal model.</p>
<p>We can further quantify the notion of "closeness" to the ideal model as the area between the ideal model and the cumulative distribution plot of a given model. Let's represent this area by $AAC$ or "area above curve$. <strong>AAC reflects how far is a given model from the ideal model or the possible improvement opportunity</strong>, For instance, in the below plot the red region indicates the AAC for the xgboost based model. Similarly, AAC of the "mean model" will be equal to green region plus red region. Mathematically, we can define AAC as:</p>
$$ AAC(m) = \int_0^{1}{|y-\hat{y}_m|} ~ \approx ~ \sum_i{|y_i-\hat{y}_{m, i}|} $$
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">test</span><span class="p">[[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">,</span> <span class="s1">&#39;xgboost_model_error&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">abs</span><span class="p">(),</span> <span class="n">value_vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">,</span> <span class="s1">&#39;xgboost_model_error&#39;</span><span class="p">])</span>
<span class="n">meanCount</span><span class="p">,</span> <span class="n">binEdges1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)])</span>
<span class="n">meanCumSum</span> <span class="o">=</span> <span class="n">meanCount</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">xgboostCount</span><span class="p">,</span> <span class="n">binEdges2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;xgboost_model_error&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">abs</span><span class="p">(),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="nb">range</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">value</span><span class="p">)])</span>
<span class="n">xgboostCumSum</span> <span class="o">=</span> <span class="n">xgboostCount</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">binEdges1</span><span class="p">,</span> <span class="n">binEdges2</span><span class="p">),</span> <span class="s2">&quot;Bins are different&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;absolute_error&#39;</span><span class="p">:</span> <span class="n">binEdges1</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">binEdges1</span><span class="p">)</span><span class="o">/</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="s1">&#39;mean_cumsum&#39;</span><span class="p">:</span> <span class="n">meanCumSum</span><span class="p">,</span>
    <span class="s1">&#39;xgboost_cumsum&#39;</span><span class="p">:</span> <span class="n">xgboostCumSum</span>
<span class="p">})</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;absolute_error&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_ribbon</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="s1">&#39;mean_cumsum&#39;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="s1">&#39;xgboost_cumsum&#39;</span><span class="p">),</span> <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">geom_ribbon</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="s1">&#39;xgboost_cumsum&#39;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">geom_ribbon</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="s1">&#39;mean_cumsum&#39;</span><span class="p">),</span> <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Absolute Error&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;Cumulative Percentage of Houses&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ggtitle</span><span class="p">(</span><span class="s2">&quot;Cumulative distribution of xgboost based model and mean model&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/01-RegressionMetrics_13_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;ggplot: (307557901)&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The advantage of AAC over MAE is that it takes into account the complete distribution of the absolute error and further quantify how far is a given model from the ideal model. However, as an evaluation metric, it's missing many desirable characteristics. Some of the key concerns are:</p>
<ol>
<li><p>AAC is inversely related to model quality. As AAC increases, model quality deteriorates and vice-versa and thereby is counter-intuitive.</p>
</li>
<li><p>AAC is bounded only on one side.  IOA can range from 0 to infinity. As the model quality deteriorates and AAC increases to infinite. A good metric should be bounded between 0 and 1 or -1 to 1 to make it easy to interpret.</p>
</li>
<li><p>AAC is not differentiable. AAC uses "absolute" function, which is non-differentiable. For optimization, it is often desirable to have a function that is differentiable.</p>
</li>
</ol>
<p>Let's focus on the first problem. One simple fix to address the inverse relationship problem is to consider the area under the curve (AUC). In the above plot, AUC for the XGboost based model is the sum of the green and the grey regions. Conceptually, AUC represents improvement made by a given model as compared to the worst model. Theoretically, the worst model has an infinite absolute error for 100% test cases. Thus, the cumulative distribution plot of it overlaps with the x-axis and extends to infinity. Using the worst model, however, complicates computing AUC and hence we need some other baseline model. The "mean model," one of the three models that we trained, is another good baseline. It uses no additional features other than the target variable itself and, in practice, is the worst model that the machine can learn. Let's redefine AUC as $AUC_{mean}$ to represent the area the cumulative distribution plot of a given model and the mean model. In the above plot, the green region represents $AUC_{mean}$ for the XGboost based model. Mathematically, we can represent $AUC_{mean}$ as</p>
$$ AUC_{mean}(m) =  \int_0^{1}{|\hat{y}_m-\bar{y}|} ~ \approx ~ \sum_i{|\hat{y}_m-\bar{y}|} $$<p>where</p>
<ul>
<li>m represents a given model, such as XGboost based model</li>
<li>$\hat{y}_m$ represents predicted target value for a given test case</li>
<li>$\bar{y}$ represents mean predicted target value.</li>
</ul>
<p>We can also rewrite $AUC_{mean}$ in terms of AAC as shown below</p>
$$ AUC_{mean}(m) = AAC(mean) - AAC(m) $$<p>where</p>
<ul>
<li>$AAC(mean)$ represents AAC for the mean model. </li>
<li>$AAC(m)$ represents AAC for a given model. </li>
</ul>
<p>$AUC_{mean}$ can range from $-\infty$ to $\infty$. It will be negative when our model is worse than the mean model and hence the cumulative plot of the given model is below the mean model. However, as discussed earlier, a bounded metric is much more easy to interpret and hence desirable. One way to bound $AUC_{mean}$ above is to normalize it by AAC(mean). That we redefine $AUC_{mean}$ as</p>
$$ AUC_{mean}(m) = \frac{AAC(mean) - AAC(m)}{AAC(mean)} $$<p>or</p>
$$ AUC_{mean}(m) = 1 - \frac{AC(m)}{AAC(mean)} $$<p>Theoretically, based on the new definition, AUC<em>{mean} can range from $-\infty$ to 1. It is 1 when area above the curve for the given model, i.e. $AAC(m)$ is equal to 0. In that case we have an ideal model that is accurate for 100% of the test cases. $AUC</em>{mean}(m)$ will be zero when the model is same as the mean model. However if its worse than the mean model then it can range from anywhere 0 to $-\infty$. In pratcise, however, $AUC_{mean}(m)$ can only range from 0 to 1. If it is less than 0, then one can essentially replace the learned model with the mean model.</p>
<p>One last issue with AAC is that its not deferentiable due to usage of the "absolute" function. We used the "absolute" function to make the errors additive. To fix this, we can replace the "absolute" function with the "square" function. Thus we can rewrite AUC definition as</p>
$$ AUC_{mean}(m) = 1 - \frac{AC(m)}{AAC(mean)} \approx 1 - \frac{\sum{(y-\hat{y})^2}}{\sum{(y-\bar{y})^2}}$$<p>The above equation should start looking familiar. Not only we rediscovered intuition behind $R^2$ but can also visualize the metric. The below code snippet computes $R^2$ based on the idea explained above and also using sklearn library.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Manually calculating R2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Manually Computing R2&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Xgboost Model: &quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;xgboost&#39;</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regression Model: &quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;regression_model_error&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Model: &quot;</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean_model_error&#39;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>


<span class="c1"># Using sklearn</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Using Sklearn&quot;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">r2_score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decision Tree: &quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;xgboost&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Regression Model: &quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Model: &quot;</span><span class="p">,</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Manually Computing R2
Xgboost Model:  0.8948728277178886
Regression Model:  0.7041704429324553
Mean Model:  0.0

Using Sklearn
Decision Tree:  0.8830617869680325
Regression Model:  0.6709339839115636
Mean Model:  -0.11235002800380478
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Note: For the mean model one might expect $R^2 = 0$. However using sklear we get non zero value. That's because sklearn is computing mean house price based on the test dataset.</p>
<p>Apart from "Mean Absolute Error" and $R^2$, there are few other metrics, such as squared error, squared log error, etc., that are popularly used when evaluating a regression model. A good starting to find more about these metrics is the list of implemented metrics in <code>sklear</code> library over <a href="http://scikit-learn.org/stable/modules/model_evaluation.html">here</a>.</p>
<p>One might also find it useful to read about adjusted $R^2$ metric. $R^2$ doesn't penalize for the number of features and is addressed by adjusted $R^2$ metric. Hopefully this chapter will now make it easy to understand some of these variants of $R^2$ and other regression metrics.</p>

</div>
</div>
</div>
</div>

 

