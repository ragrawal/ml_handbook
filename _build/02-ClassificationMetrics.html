---
redirect_from:
  - "/02-classificationmetrics"
interact_link: content/02-ClassificationMetrics.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Classification
prev_page:
  url: /01-RegressionMetrics.html
  title: |-
    Regression
next_page:
  url: /03-GradientDescent.html
  title: |-
    Basic Intution (Part I)
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Evaluating-Classification-Problems">Evaluating Classification Problems<a class="anchor-link" href="#Evaluating-Classification-Problems"> </a></h1><p>Classification problems are the problems were the desired output is a categorical value. For instance, learning to detect different kinds of objects such as car, bicycle, etc., predicting whether someone has a cancer or not, etc are few examples of classification problem. Similar to <a href="01-RegressionMetrics.ipynb">Chapter 1</a>, the objective of this chapter is to understand the different metrics used to evaluate classification models. We start with an open source breast cancer dataset and build a classifier. Thereafter, we try to intutively develop various metrics such as accuracy, precision, recall, and area-under-curve to compare and evaluate the classifier. Eventually, we look how to generalize these metrics from evaluating binary classification problem such as the breast cancer dataset to multi-categorical (or multiclass) problem.</p>
<h2 id="Predicting-Cancer">Predicting Cancer<a class="anchor-link" href="#Predicting-Cancer"> </a></h2><p>Assuming we have been asked to build a model that can predict whether a person has a breast cancer or not based on various features as described in the following <a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29">dataset</a>.
In order to train the classification model, we randomly divide the dataset into training and testing dataset. As shown in the code snippet below, the training dataset is then used to build a random forest classifier. Finally we try to evaluate the performance of the classifier using the test dataset.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import packages</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">plotnine</span> <span class="k">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span> <span class="c1">#, module=&quot;scipy&quot;, message=&quot;^internal gelsd&quot;)</span>

<span class="c1"># load dataset</span>
<span class="n">cancer</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;hasCancer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">cancer</span><span class="o">.</span><span class="n">target</span>
<span class="n">data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>hasCancer</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>106</th>
      <td>11.640</td>
      <td>18.33</td>
      <td>75.17</td>
      <td>412.5</td>
      <td>0.11420</td>
      <td>0.10170</td>
      <td>0.07070</td>
      <td>0.034850</td>
      <td>0.1801</td>
      <td>0.06520</td>
      <td>...</td>
      <td>29.26</td>
      <td>85.51</td>
      <td>521.7</td>
      <td>0.1688</td>
      <td>0.2660</td>
      <td>0.28730</td>
      <td>0.12180</td>
      <td>0.2806</td>
      <td>0.09097</td>
      <td>1</td>
    </tr>
    <tr>
      <th>13</th>
      <td>15.850</td>
      <td>23.95</td>
      <td>103.70</td>
      <td>782.7</td>
      <td>0.08401</td>
      <td>0.10020</td>
      <td>0.09938</td>
      <td>0.053640</td>
      <td>0.1847</td>
      <td>0.05338</td>
      <td>...</td>
      <td>27.66</td>
      <td>112.00</td>
      <td>876.5</td>
      <td>0.1131</td>
      <td>0.1924</td>
      <td>0.23220</td>
      <td>0.11190</td>
      <td>0.2809</td>
      <td>0.06287</td>
      <td>0</td>
    </tr>
    <tr>
      <th>144</th>
      <td>10.750</td>
      <td>14.97</td>
      <td>68.26</td>
      <td>355.3</td>
      <td>0.07793</td>
      <td>0.05139</td>
      <td>0.02251</td>
      <td>0.007875</td>
      <td>0.1399</td>
      <td>0.05688</td>
      <td>...</td>
      <td>20.72</td>
      <td>77.79</td>
      <td>441.2</td>
      <td>0.1076</td>
      <td>0.1223</td>
      <td>0.09755</td>
      <td>0.03413</td>
      <td>0.2300</td>
      <td>0.06769</td>
      <td>1</td>
    </tr>
    <tr>
      <th>114</th>
      <td>8.726</td>
      <td>15.83</td>
      <td>55.84</td>
      <td>230.9</td>
      <td>0.11500</td>
      <td>0.08201</td>
      <td>0.04132</td>
      <td>0.019240</td>
      <td>0.1649</td>
      <td>0.07633</td>
      <td>...</td>
      <td>19.62</td>
      <td>64.48</td>
      <td>284.4</td>
      <td>0.1724</td>
      <td>0.2364</td>
      <td>0.24560</td>
      <td>0.10500</td>
      <td>0.2926</td>
      <td>0.10170</td>
      <td>1</td>
    </tr>
    <tr>
      <th>563</th>
      <td>20.920</td>
      <td>25.09</td>
      <td>143.00</td>
      <td>1347.0</td>
      <td>0.10990</td>
      <td>0.22360</td>
      <td>0.31740</td>
      <td>0.147400</td>
      <td>0.2149</td>
      <td>0.06879</td>
      <td>...</td>
      <td>29.41</td>
      <td>179.10</td>
      <td>1819.0</td>
      <td>0.1407</td>
      <td>0.4186</td>
      <td>0.65990</td>
      <td>0.25420</td>
      <td>0.2929</td>
      <td>0.09873</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 31 columns</p>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># split data into training and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train: &quot;</span><span class="p">,</span> <span class="n">train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test: &quot;</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of Positive Cases In Test Dataset: &quot;</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="n">test</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of Negative Cases In Test Dataset: &quot;</span><span class="p">,</span> <span class="n">test</span><span class="p">[</span><span class="n">test</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train:  (455, 31)
Test:  (114, 31)
Number of Positive Cases In Test Dataset:  75
Number of Negative Cases In Test Dataset:  39
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">[</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">],</span> <span class="n">train</span><span class="p">[</span><span class="s1">&#39;hasCancer&#39;</span><span class="p">])</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Accuracy-as-a-Metric">Accuracy as a Metric<a class="anchor-link" href="#Accuracy-as-a-Metric"> </a></h2><p>Once we have trained a classifier, the next challenge is how to evaluate its performance. To start with, one of the most intutive metric we can think of is "accuracy" i.e. percentage of test cases for which the classifier's prediction is correct. Mathematically, accuracy can be represented as 
$$ acc = \frac{|T|}{|T| + |F|} $$
where</p>
<ul>
<li>$|T|$ is the number of test cases where the actual class and the predicted class match. </li>
<li>$|F|$ is the number of test cases where the actual class and the predicted class differ.</li>
</ul>
<p>Below code snippet shows how to calculate accuracy manually and using sklearn.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># count number of predictions that are True i.e. predicted value == actual value</span>
<span class="n">true_results</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">test</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="n">test</span><span class="o">.</span><span class="n">prediction</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">false_results</span> <span class="o">=</span> <span class="n">test</span><span class="p">[</span><span class="n">test</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">!=</span> <span class="n">test</span><span class="o">.</span><span class="n">prediction</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># compute accuracy </span>
<span class="n">accuracy1</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">true_results</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">true_results</span> <span class="o">+</span> <span class="n">false_results</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy (manually): </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy1</span><span class="p">))</span>

<span class="c1"># compute accuracy using sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">accuracy_score</span>
<span class="n">accuracy2</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;hasCancer&#39;</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;prediction&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy (sklearn): </span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy2</span><span class="p">))</span>

<span class="c1"># assert two accuracy are same</span>
<span class="k">assert</span> <span class="n">accuracy1</span> <span class="o">==</span> <span class="n">accuracy2</span><span class="p">,</span> <span class="s2">&quot;Accuracy don&#39;t match&quot;</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Accuracy (manually): 0.97
Accuracy (sklearn): 0.97
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Accuracy as metric suffers from two limitations. First, it can be misleading if you have a skewed dataset. Second, it's a point estimate of the performance of the model. The two points are further elaborated in the following sub sections.</p>
<h3 id="Skewed-Dataset">Skewed Dataset<a class="anchor-link" href="#Skewed-Dataset"> </a></h3><p>Assume we are provided with a home loan application dataset and asked to build a classifier that can predict whether a home loan application is likely to get defaulted or not in future. In general, the home loan default rate is less than 1% and let's assume our dataset is representative of the population, i.e., only 1% of all the home application in our dataset have been defaulted. Now, if somebody approaches us that his/her classifier is able to achieve an accuracy of 99% of the above given dataset, it shouldn't suprise you. Given the skeweness, it can be achieved even by a constant classifier that always returns 0 i.e. the application is not likely to be defaulted. This constant classifier will be correct 99% of the time. <strong>Thus, in the case of a skewed dataset, accuracy as a metric can be misleading.</strong></p>
<p>One way to overcome the skewed dataset issue is to measure accuracy for each individual categories. This idea leads us to confusion matrix. As shown in the table below, a confusion matrix is a simple matrix where one axes contains actual categories and the other contains predicted categories. The cells contains number of test cases falling in a particular combination of actual and predicated categories.</p>
<table>
<thead><tr>
<th style="text-align:center">_</th>
<th style="text-align:center"><strong>Predicted Positive</strong></th>
<th style="text-align:center"><strong>Predicted Negative</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Actual Positive</strong></td>
<td style="text-align:center">True Positive (FP)</td>
<td style="text-align:center">False Negatives (FN)</td>
</tr>
<tr>
<td style="text-align:center"><strong>Actual Negative</strong></td>
<td style="text-align:center">False Positive (FP)</td>
<td style="text-align:center">True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>An easy way to remember the nomeclature  (TP, FN, FP and TN) is to remember that</p>
<ol>
<li>T/F indicates whether the actual class and the predicted class matches or not. </li>
<li>P/N indicates the predicted category. </li>
</ol>
<p>Below code snippets computes shows how to compute confusion matrix based on the model we trained above for the breast cancer dataset.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;hasCancer&#39;</span><span class="p">,</span> <span class="s1">&#39;prediction&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;NumTestCases&#39;</span><span class="p">})</span>
<span class="n">cm</span><span class="o">.</span><span class="n">pivot_table</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="s1">&#39;hasCancer&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;prediction&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">&#39;NumTestCases&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>prediction</th>
      <th>0</th>
      <th>1</th>
    </tr>
    <tr>
      <th>hasCancer</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>38</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>73</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Based on the above matrix, we can construct many different interesting matrix. For instance, we can notice that our trained classifier for the breast cancer dataset is able to detect 97% (73 of 75) positive cases correctly. This is known as True Positive Rate (TPR). TPR is also referred as sensitivity or recall. We can also compute what percentage of negative cases have been classified as positive. This is known as false positive rate or FPR. Based on the above confusion matrix we known that FPR is $1/39$ or about 2.5%. There are many other metrics defined based on confusion matrix and you can find a good summary of those over <a href="https://en.wikipedia.org/wiki/Confusion_matrix">here</a>.</p>
<h3 id="Point-Estimate">Point Estimate<a class="anchor-link" href="#Point-Estimate"> </a></h3><p>Classifiers usually assign probability scores to each of the output categories and the category with the highest probability score is usually treated as the predicted category. As shown below, one can examine probability score of individual categories using <code>predict_proba</code> function. Notice that the final prediction correspond to the class with the highest probability score.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">])</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;prob_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">test</span><span class="p">[</span><span class="s1">&#39;prob_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span> <span class="o">=</span> <span class="n">probs</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">test</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">3</span><span class="p">)[[</span><span class="s1">&#39;prob_0&#39;</span><span class="p">,</span> <span class="s1">&#39;prob_1&#39;</span><span class="p">,</span> <span class="s1">&#39;prediction&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">({</span>
    <span class="s1">&#39;prob_0&#39;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="s1">&#39;prob_1&#39;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="si">{:.2f}</span><span class="s2">&quot;</span><span class="p">,</span>
<span class="p">})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<style  type="text/css" >
</style><table id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470" ><thead>    <tr>        <th class="blank level0" ></th>        <th class="col_heading level0 col0" >prob_0</th>        <th class="col_heading level0 col1" >prob_1</th>        <th class="col_heading level0 col2" >prediction</th>    </tr></thead><tbody>
                <tr>
                        <th id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470level0_row0" class="row_heading level0 row0" >222</th>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row0_col0" class="data row0 col0" >0.01</td>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row0_col1" class="data row0 col1" >0.99</td>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row0_col2" class="data row0 col2" >1</td>
            </tr>
            <tr>
                        <th id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470level0_row1" class="row_heading level0 row1" >533</th>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row1_col0" class="data row1 col0" >1.00</td>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row1_col1" class="data row1 col1" >0.00</td>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row1_col2" class="data row1 col2" >0</td>
            </tr>
            <tr>
                        <th id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470level0_row2" class="row_heading level0 row2" >193</th>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row2_col0" class="data row2 col0" >0.50</td>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row2_col1" class="data row2 col1" >0.50</td>
                        <td id="T_aff6f6e8_5ae1_11e9_88a6_f218984e9470row2_col2" class="data row2 col2" >1</td>
            </tr>
    </tbody></table>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case of binary classifier, the predicted category naturally has probability score more than 0.5. Thus, we can say that $y=1$ only if $P(y=1|x) &gt; 0.5$, otherwise its $y=0$. We can further generalize above condition by replacing static 0.5 with a threshold, say $\tau$. Thus, $y=1$ only if $P(y=1|x) &gt; \tau$, otherwise its $y=0$.</p>
<p>Introducing $\tau$ has an advantage. By changing $\tau$ we can influence TPR, FPR, etc and thereby influence the same model to have higher precision or recall. For instance, in the case of breast cancer dataset, let's assume that the cost of false negative (i.e predicting negative when one has a breast cancer) is very high and thereby one of the requirements is to have false positive rate less than 1%. From the above confusion matrix, we can examine that the existing trained model has false negative rate (FNR) of $2/75 \approx 2.5%$. One simple way to meet the requirement of having FNR is to lower down $\tau$. By reducing the value of $\tau$, we will allow more samples to be predicted as positives and thereby decreases FNR. Below code snippet demonstrates this by changing $\tau$ from 0 to 1 in step of 0.01.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">point_estimates</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes various metrics for a given threshold value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="k">def</span> <span class="nf">_compute</span><span class="p">(</span><span class="n">threshold</span><span class="p">):</span>
        
        <span class="n">tp</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">prob_1</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">fp</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">prob_1</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="n">df</span><span class="p">[(</span><span class="n">df</span><span class="o">.</span><span class="n">prob_1</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span>
            <span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">prob_1</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">1</span><span class="p">))</span>
            <span class="o">|</span> <span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">prob_1</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">hasCancer</span> <span class="o">==</span> <span class="mi">0</span><span class="p">))</span>
        <span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">threshold</span><span class="p">,</span> <span class="n">tp</span> <span class="o">/</span> <span class="n">p</span><span class="p">,</span> <span class="n">fp</span> <span class="o">/</span> <span class="n">n</span><span class="p">,</span> <span class="n">fn</span> <span class="o">/</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="o">/</span> <span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="n">n</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">_compute</span>
    
<span class="n">estimator</span> <span class="o">=</span> <span class="n">point_estimates</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="p">[</span><span class="n">estimator</span><span class="p">(</span><span class="n">threshold</span><span class="o">/</span><span class="mi">1000</span><span class="p">)</span> <span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="s1">&#39;fpr&#39;</span><span class="p">,</span> <span class="s1">&#39;fnr&#39;</span><span class="p">,</span> <span class="s1">&#39;accuracy&#39;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;threshold&#39;</span><span class="p">))</span>

<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s1">&#39;threshold&#39;</span><span class="p">),</span> <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">colour</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">))</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Threshold&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">theme_linedraw</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">theme</span><span class="p">(</span><span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>threshold</th>
      <th>tpr</th>
      <th>fpr</th>
      <th>fnr</th>
      <th>accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>102</th>
      <td>0.102</td>
      <td>1.000000</td>
      <td>0.410256</td>
      <td>0.000000</td>
      <td>0.859649</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.192</td>
      <td>1.000000</td>
      <td>0.333333</td>
      <td>0.000000</td>
      <td>0.885965</td>
    </tr>
    <tr>
      <th>517</th>
      <td>0.517</td>
      <td>0.973333</td>
      <td>0.000000</td>
      <td>0.026667</td>
      <td>0.982456</td>
    </tr>
    <tr>
      <th>834</th>
      <td>0.834</td>
      <td>0.866667</td>
      <td>0.000000</td>
      <td>0.133333</td>
      <td>0.912281</td>
    </tr>
    <tr>
      <th>847</th>
      <td>0.847</td>
      <td>0.866667</td>
      <td>0.000000</td>
      <td>0.133333</td>
      <td>0.912281</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/02-ClassificationMetrics_11_1.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;ggplot: (314777506)&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Area-Under-Curve">Area Under Curve<a class="anchor-link" href="#Area-Under-Curve"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As discussed above, accuracy and other metrics are point estimates. This creates a problem. For instance, consider two models both have same accuracy at 0.5. But one model has a higher accuracy when $\tau = 0.25$ and another has a higher accuracy when $\tau = 0.75$. How do you decide which one is better. For a moment let's think about an ideal model.</p>
<p>As shown in the below plot (grey region), one might notice that TRP and FPR curves form a closed region. Now for a momemnt think about an ideal model and how the region bounded by TPR and FPR curve will look like ? An ideal model will be one that definitively distinguishes positive and negative test cases and is accurate 100%. Thus, an ideal will be the one that assigns a probability of 1 to positive test cases and 0 for negative test cases. As a result, TPR will be always 100% irrespective what threshold value we choose. Thus, TPR will be a horizontal line intersecting y axis at 100% point. Similarly, for an ideal model FPR will be always 0% and therefore it will be a horizontal line intersecting y-axis at 0%. TPR and FPR curves for an ideal model then form the bounding box for the below shown plot.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="p">(</span>
    <span class="n">ggplot</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span>
        <span class="n">aes</span><span class="p">(</span><span class="n">group</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">colour</span><span class="o">=</span><span class="s1">&#39;variable&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;value&#39;</span><span class="p">),</span>
        <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;threshold&#39;</span><span class="p">,</span> <span class="s1">&#39;fpr&#39;</span><span class="p">,</span> <span class="s1">&#39;tpr&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">id_vars</span><span class="o">=</span><span class="s1">&#39;threshold&#39;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="o">+</span> <span class="n">geom_ribbon</span><span class="p">(</span>
        <span class="n">aes</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="s1">&#39;fpr&#39;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;threshold&#39;</span><span class="p">),</span> 
        <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;lightgrey&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span>
    <span class="p">)</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;Threshold&quot;</span><span class="p">)</span> <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;Value&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">theme_linedraw</span><span class="p">()</span>
    <span class="o">+</span> <span class="n">theme</span><span class="p">(</span><span class="n">figure_size</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/02-ClassificationMetrics_14_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;ggplot: (314866906)&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Based on the above discussion, it is easy to notice that as the quality of our model improves the TPR curve should move towards the top left corner and FPR should move towards the bottom right corner. In other words, the quality of our trained model improves as the area between the curves increases. This essentially forms the intution behind the Area Under Curve (AUC) metric.</p>
<p>In pratice, however there is one small difference. As shown below, instead of plotting two separate curves (TPR and FPR) as a function of threshold, we can plot TPR as a function of FPR. As the model quality improves, the line will move towards top right corner and the area under the curve will increase. Similar to $R^2$ metric discussed in <a href="01-RegressionMetrics.ipynb">chapter 1</a>, the region above the curve (red colored region) is the opportunity area to improve our model.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fpr_tpr_curve</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;fpr&#39;</span><span class="p">)[</span><span class="s1">&#39;tpr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="p">(</span>
    <span class="n">ggplot</span><span class="p">(</span>
        <span class="n">fpr_tpr_curve</span><span class="p">,</span>
        <span class="n">aes</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;fpr&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;tpr&#39;</span><span class="p">)</span>
    <span class="p">)</span> 
<span class="c1">#     + geom_ribbon(aes(ymax=&#39;tpr&#39;, ymin=&#39;fpr&#39;), fill=&#39;green&#39;)</span>
    <span class="o">+</span> <span class="n">geom_ribbon</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">ymin</span><span class="o">=</span><span class="s1">&#39;tpr&#39;</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">fill</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span> 
    <span class="o">+</span> <span class="n">geom_line</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">xlab</span><span class="p">(</span><span class="s2">&quot;False Positive Rate&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">ylab</span><span class="p">(</span><span class="s2">&quot;True Positive Rate&quot;</span><span class="p">)</span>
    <span class="o">+</span> <span class="n">theme_linedraw</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea ">
<img src="images/02-ClassificationMetrics_16_0.png"
>
</div>

</div>
</div>
<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;ggplot: (314771078)&gt;</pre>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below code snippet computes the area under curve and validates that this what we get from sklearn.</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="k">import</span> <span class="n">roc_auc_score</span>
<span class="n">auc_manually</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">trapz</span><span class="p">(</span><span class="n">fpr_tpr_curve</span><span class="p">[</span><span class="s1">&#39;tpr&#39;</span><span class="p">],</span> <span class="n">x</span><span class="o">=</span><span class="n">fpr_tpr_curve</span><span class="p">[</span><span class="s1">&#39;fpr&#39;</span><span class="p">])</span>
<span class="n">auc_sklearn</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="s1">&#39;hasCancer&#39;</span><span class="p">],</span> <span class="n">test</span><span class="p">[</span><span class="s1">&#39;prob_1&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Area Under Curve (manually): </span><span class="si">{:.3}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auc_manually</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Area Under Curve (Sklearn): </span><span class="si">{:.3}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auc_sklearn</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Area Under Curve (manually): 0.998
Area Under Curve (Sklearn): 0.998
</pre>
</div>
</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="$F_1$-and-Variants">$F_1$ and Variants<a class="anchor-link" href="#$F_1$-and-Variants"> </a></h2>
</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Another popular metric that is often used to evaluate classifiers is $F_1$ score and it's variant. $F_1$ is described as the harmonic mean of precision and recall. To better understand the definition, first let's try to understand what is precision and recall.</p>
<p>Often when dealing with a classification problem, we are interested in the positive cases and how well the trained model is able to detect these positive test cases. In other words we are interested in true positives (TP). However, the number of TP by themself doesn't make sense. For instance, saying there are 50 TP doesn't communicate any insightful information. In order to evaluate the quality of the model in reference to true positives, we need to normalize the number of true positive cases with the number of positive cases. For instance, it makes more sense if one says that the model is able to detect 50 of 200 positive cases or true positive rate is 25% of actual positive cases.</p>
<p>There is a however another challenge. We can talk about the total number of positive cases, used for normalizing true positiives, from two different perspectives: from the data perspective or from the model perspective. From the data perspective, the total number of positives cases is the number of cases that are actually labeled as positives. Based on the confusion matrix, the total number of positive cases from the data perspective is the summation of true positives (TP) and false negatives (FN). <strong>True positive rate from the data perspecrtive is known as "recall"</strong>. Mathematically, it can be represented as:</p>
$$ Recall = \frac{TP}{TP + FN}$$<p>In contrast to recall, <strong>precision is true positive rate from the model perspective</strong>. The total number of positive cases is the number of cases that the model predicts to be positive. Based on the confusion matrix, it can be represented as:</p>
$$ Precision = \frac{TP}{TP + FP} $$<p>Notice that in the formula for recall and precision only the denomoniator is different. One is the total number of actual positive cases in the test dataset and another is the total number of predicted positive cases, respectively.</p>
<p>Now we have two different ways to talk about true positive rates. However as a metric it we would like to have a single number. One simple way to combine these two different true positive rates is to take the average. When dealing with ratio, harmonic mean makes more sense than arithmetic mean. This harmonic mean of precision and recall is termed as $F_1$ score and written as:</p>
$$ F_1 = \frac{2}{\frac{1}{Precision} + \frac{1}{Recall}} = \frac{2 \times Precision \times Recall}{Precsiion + Recall } $$
</div>
</div>
</div>
</div>

 

