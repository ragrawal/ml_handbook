---
redirect_from:
  - "/04-linearregression"
interact_link: content/04-LinearRegression.ipynb
kernel_name: python3
has_widgets: false
title: |-
  Linear Regression (Part II)
prev_page:
  url: /03-GradientDescent.html
  title: |-
    Basic Intution (Part I)
next_page:
  url: 
  title: |-
    
comment: "***PROGRAMMATICALLY GENERATED, DO NOT EDIT. SEE ORIGINAL FILES IN /content***"
---
<div class="jb_cell">

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Gradient-Descent:-Part-II">Gradient Descent: Part II<a class="anchor-link" href="#Gradient-Descent:-Part-II"> </a></h1><p>In chapter 3, we looked through the basic idea of gradient descent and how it can help to find the optimal point for a given function. In this chapter, I will walk through how the algorithm we developed in chapter 3 can be used for linear regression. However, this transition is quite a jump in thought process. Essentially, spend some time thinking about the the following challenge.</p>
<div style="background:lightyellow; padding:5px;">
In chapter 3, we knew the function for which we were exploring the optimal point. But in the case of linear regression we only have bunch of data points (X, y) and we trying to estimate the underlying function that leads to y given a set of parameters X. So, one has to think about
<ol>
    <li>how this related to miminization problem?, and </li>
    <li>what function are we trying to minimize?</li>
</ol>
</div><p>If you have skipped Chapter 1 or forgot the details, this might be a great time to review it. As I said earlier, the objective of linear regression is to estimate the underlying function that given a set of parameters X can generate y. As I discussed in chapter 1, one can come up with any random function but how do we evaluate the validate that this is a reasonalable function. One of the evaluation metric discussed in chapter 1 is <strong>Mean Square Error or MSE</strong>. It's the mean square distance between the predicted and actual target value. Mathematically, it is written as:</p>
$$ MSE = \frac{1}{m}\sum_{i=1}^{m}(y_i - \hat{y_i})^2 $$<p>where</p>
<ul>
<li>$m$ represents number of data points</li>
<li>$\hat{y_i}$ represents predicted target value</li>
<li>$y_i$ represents actual target value</li>
</ul>
<p>if MSE is a good metric to evaluate the quality of a model, a natural thought would be why not focus on building a model that minimizes MSE. Great, atleast now we are talking about some minimization problem and also have a function, MSE, that is differentiable. Thus, we can leverage Gradient descent algorithm. But still there is a problem. $m$ and $y_i$ are known values from the given dataset but we don't know how to compute $\hat{y_i}$. This is where linear regression gets interesting. We can compute $\hat{y_i}$ using random function as long as it's a linear combination of input parameters (though the parameters themselves can be exponential, log, etc). To make this understanding concrete, let's assume we have 1000 data points with the underlying function being $y = 4x_1^2 + 2x_2^2 + \$.</p>
<p>try to show how the idea of gradient descent can be used in linear regression. Broadly speaking, in linear regression we have bunch of observations and we want to discover the underlying function that has generated these observations. For instance, similar to previous</p>

</div>
</div>
</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="k">import</span> <span class="n">normal</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">random</span>

<span class="n">dataDF</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
  <span class="s1">&#39;x1&#39;</span><span class="p">:</span>  <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)],</span>
  <span class="s1">&#39;x2&#39;</span><span class="p">:</span>  <span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)],</span>    
  <span class="s1">&#39;noise&#39;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>  
<span class="p">})</span>
<span class="n">dataDF</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">dataDF</span><span class="p">[</span><span class="s1">&#39;x1&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dataDF</span><span class="p">[</span><span class="s1">&#39;x2&#39;</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">dataDF</span><span class="p">[</span><span class="s1">&#39;noise&#39;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="o">.</span><span class="n">scatter_3d</span><span class="p">(</span><span class="n">dataDF</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;x2&#39;</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">to_image</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">Image</span>
<span class="n">Image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="c1"># fig.show() -- use this command for generating interactive image</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">



<div class="output_png output_subarea output_execute_result">
<img src="images/04-LinearRegression_1_0.png"
>
</div>

</div>
</div>
</div>
</div>

</div>
</div>

<div class="jb_cell">

<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">copy</span>
<span class="kn">from</span> <span class="nn">scipy.spatial</span> <span class="k">import</span> <span class="n">distance</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="k">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="k">import</span> <span class="n">HTML</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="n">num_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_prime</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span><span class="o">*</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">diff</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_prime</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">gradient</span>


<span class="k">def</span> <span class="nf">ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="mf">0.03</span><span class="p">,</span> <span class="n">fun</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span> <span class="n">max_iterations</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">traceback</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">stopping_threshold</span> <span class="o">=</span> <span class="mf">1.0e-6</span><span class="p">):</span>
    
    <span class="c1"># number of samples</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># number of parameters</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># starting point -- randomly select</span>
    <span class="n">theta1</span> <span class="o">=</span> <span class="p">[</span><span class="n">random</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span>
    
    <span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iterations</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">traceback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">traceback</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="p">(</span><span class="n">theta1</span><span class="p">))</span>

        <span class="n">theta2</span>  <span class="o">=</span> <span class="n">theta1</span> <span class="o">-</span> <span class="n">eta</span> <span class="o">*</span> <span class="n">gradient</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta1</span><span class="p">)</span>
        
        <span class="c1"># check if we reached stopping criteria threshold</span>
        <span class="k">if</span> <span class="n">distance</span><span class="o">.</span><span class="n">euclidean</span><span class="p">(</span><span class="n">theta2</span><span class="p">,</span> <span class="n">theta1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">stopping_threshold</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">theta1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">theta1</span> <span class="o">=</span> <span class="n">theta2</span>
        
    <span class="c1"># if we reached max iterations then return current point</span>
    <span class="k">return</span> <span class="n">theta1</span>



<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">dataDF</span><span class="p">[[</span><span class="s1">&#39;x1&#39;</span><span class="p">,</span> <span class="s1">&#39;x2&#39;</span><span class="p">]]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataDF</span><span class="p">[</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">traceback</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">ols</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">max_iterations</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">traceback</span><span class="o">=</span><span class="n">traceback</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&lt;strong&gt;Optimized Parameters: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">theta</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="jb_output_wrapper }}">
<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<strong>Optimized Parameters: [3.99997318 2.00002011]
</div>

</div>
</div>
</div>
</div>

</div>
</div>

 

